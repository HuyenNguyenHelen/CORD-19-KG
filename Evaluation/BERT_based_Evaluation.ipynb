{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT-based Evaluation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1awRlBQkgnKE-VMzDNaXL2lYcIC2wGTXv",
      "authorship_tag": "ABX9TyNPsQ6TJ5rw+kY53x95rf8R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HuyenNguyenHelen/CORD-19-KG/blob/master/Evaluation/BERT_based_Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT-based Evaluation: correctness, and relatedness\n",
        "Use the saved BERT model to automatically label correctness for entities and triples, and relatedness fo entities in the ten KGs."
      ],
      "metadata": {
        "id": "6zKA5Z8H4mss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZ-52Rg2-nJt",
        "outputId": "e7b1ebfc-c158-4ba0-f981-c9f1225a831a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_pretrained_bert pytorch-nlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SGEKQba-pCl",
        "outputId": "8a14197c-ea69-4302-9cb9-b36bff6e62c3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch_pretrained_bert\n",
            "  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 31.2 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████                        | 30 kB 7.2 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40 kB 3.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51 kB 3.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 123 kB 3.9 MB/s \n",
            "\u001b[?25hCollecting pytorch-nlp\n",
            "  Downloading pytorch_nlp-0.5.0-py3-none-any.whl (90 kB)\n",
            "\u001b[K     |████████████████████████████████| 90 kB 10.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.23.1-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 43.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.11.0+cu113)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (4.64.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.21.6)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (4.2.0)\n",
            "Collecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.2-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 11.2 MB/s \n",
            "\u001b[?25hCollecting botocore<1.27.0,>=1.26.1\n",
            "  Downloading botocore-1.26.1-py3-none-any.whl (8.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.7 MB 69.0 MB/s \n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.27.0,>=1.26.1->boto3->pytorch_pretrained_bert) (2.8.2)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 74.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.27.0,>=1.26.1->boto3->pytorch_pretrained_bert) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2021.10.8)\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 73.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert, pytorch-nlp\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed boto3-1.23.1 botocore-1.26.1 jmespath-1.0.0 pytorch-nlp-0.5.0 pytorch-pretrained-bert-0.6.2 s3transfer-0.5.2 urllib3-1.25.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_pretrained_bert import BertModel\n",
        "from torch import nn\n",
        "from pytorch_pretrained_bert import BertTokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.optim import Adam\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from IPython.display import clear_output"
      ],
      "metadata": {
        "id": "NicpOxUM-rOM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgOTC1UO4lHR"
      },
      "outputs": [],
      "source": [
        "# Define BERT model\n",
        "class BertBinaryClassifier(nn.Module):\n",
        "    def __init__(self, dropout=0.1):\n",
        "        super(BertBinaryClassifier, self).__init__()\n",
        "\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear = nn.Linear(768, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "    def forward(self, tokens, masks=None):\n",
        "        # First Layer\n",
        "        _, pooled_output = self.bert(tokens, attention_mask=masks, output_all_encoded_layers=False)\n",
        "\n",
        "        dropout_output = self.dropout(pooled_output)\n",
        "\n",
        "        linear_output = self.linear(dropout_output)\n",
        "        \n",
        "        # output layer\n",
        "        proba = self.sigmoid(linear_output)\n",
        "        \n",
        "        return proba\n",
        "\n",
        "# Create main function\n",
        "def main(saved_model_path, data_path, correct_compreh = 'correct_ent' ):\n",
        "  \"\"\"\n",
        "  correct_compreh: 'correct_ent', 'correct_trip', 'compreh'\n",
        "  \"\"\"\n",
        "  # Loading data\n",
        "  \n",
        "  with open(data_path, 'r') as f:\n",
        "    data_test = pd.read_csv(f)\n",
        "  \n",
        "  if correct_compreh == 'correct_ent':\n",
        "    X_test = data_test['subject'] + data_test['object']\n",
        "\n",
        "    # Tokenizer \n",
        "  tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "  # get max len in tokenized train text to set the tokens length in the next step\n",
        "  MAX_LEN = max(map(len, X_test))  # can do len(max(X_train, key=len)) also\n",
        "  print('MAX LEN of trainning sentence is:', MAX_LEN, '\\nMAX LEN > 512 is ', MAX_LEN>512)\n",
        "\n",
        "  # Update MAX LEN if it's > 512, set it to be 225 \n",
        "  ## 512 is is the maximum seq len of BERT_BASE. But we cannot allow the seq len to be 512 since we'll run out of GPU memory --> Use max len of 225\n",
        "  MAX_LEN = 225 if MAX_LEN > 512 else MAX_LEN\n",
        "\n",
        "  # Convert to tokens using tokenizer\n",
        "  test_tokens  = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[: MAX_LEN] + ['[SEP]'], X_test.to_list()))\n",
        "\n",
        "  print( '\\nNumber of Testing Sequences:', len(test_tokens) )\n",
        "  # Following is to convert List of words to list of numbers. (Words are replaced by their index in dictionar)\n",
        "  test_tokens_ids  = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, test_tokens)),  maxlen= MAX_LEN, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
        "  # Mask the paddings with 0 and words with 1\n",
        "  test_masks = [[float(i > 0) for i in ii] for ii in test_tokens_ids]\n",
        "\n",
        "  ## Converting test token ids, test labels and test masks to a tensor and the create a tensor dataset out of them.\n",
        "  # Convert token ids to tensor \n",
        "  test_tokens_tensor = torch.tensor(test_tokens_ids)\n",
        "\n",
        "  # Convert labels to tensors\n",
        "  # test_y_tensor = torch.tensor(y_test.to_numpy().reshape(-1, 1)).float()\n",
        "\n",
        "  # Convert to tensor for maks\n",
        "  test_masks_tensor = torch.tensor(test_masks)\n",
        "\n",
        "  # Load Token, token mask and label into Dataloader\n",
        "  test_dataset = TensorDataset(test_tokens_tensor, test_masks_tensor)\n",
        "\n",
        "  # Define sampler\n",
        "  test_sampler = SequentialSampler(test_dataset)\n",
        "\n",
        "  # Define test data loader\n",
        "  test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=16)\n",
        "\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  bert_clf = BertBinaryClassifier()\n",
        "  bert_clf.load_state_dict(torch.load(saved_model_path),  strict=False)\n",
        "\n",
        "  bert_clf.eval()     # Define eval\n",
        "  bert_predicted = [] # To Store predicted result\n",
        "  all_logits = []     # Predicted probabilities that is between 0 to 1 is stored here\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for step_num, batch_data in enumerate(test_dataloader):\n",
        "\n",
        "          # Load the batch on gpu memory\n",
        "          token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n",
        "\n",
        "          # Calculate ouput of bert\n",
        "          logits = bert_clf(token_ids, masks)\n",
        "\n",
        "          # Get the numpy logits\n",
        "          numpy_logits = logits.cpu().detach().numpy()  # Detach from the GPU memory\n",
        "          \n",
        "          # Using the threshold find binary \n",
        "          bert_predicted += list(numpy_logits[:, 0] > 0.5)  # Threshold conversion\n",
        "          # all_logits += list(numpy_logits[:, 0])\n",
        "  print(bert_predicted)\n",
        "\n",
        "if __name__=='__main__':\n",
        "  main(saved_model_path = '/content/drive/MyDrive/KG/KG_EVAL_SAVE_MODEL/all.h5', \n",
        "       data_path = '/content/drive/MyDrive/KG/KG_10fold_data/subset_9.csv'  )\n"
      ]
    }
  ]
}