{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e3721e7",
   "metadata": {},
   "source": [
    "# Evaluate KG on the QA task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97429b05",
   "metadata": {},
   "source": [
    "## Process the groundtruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ac28fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dddc068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QueryID</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q1</td>\n",
       "      <td>Membrane Protein (M Protein)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q1</td>\n",
       "      <td>Envelope Protein (E Protein)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q1</td>\n",
       "      <td>Nucleocapsid Protein (N Protein) and Viral RNA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q1</td>\n",
       "      <td>Spike Protein (S Protein)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q2</td>\n",
       "      <td>Bat,[horseshoe bat]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  QueryID                                          answer\n",
       "0      Q1                    Membrane Protein (M Protein)\n",
       "1      Q1                    Envelope Protein (E Protein)\n",
       "2      Q1  Nucleocapsid Protein (N Protein) and Viral RNA\n",
       "3      Q1                       Spike Protein (S Protein)\n",
       "4      Q2                             Bat,[horseshoe bat]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the groundtruth\n",
    "with open(r\"C:\\Users\\huyen\\OneDrive\\Documents\\GitHub\\CORD-19-KG\\Evaluation\\groundtruth\\expert_answers.csv\", 'r') as f:\n",
    "    gtruth = pd.read_csv(f)\n",
    "gtruth.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17a01807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>QueryID</th>\n",
       "      <th>answer</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q1</td>\n",
       "      <td>[Membrane Protein (M Protein), Envelope Protei...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q2</td>\n",
       "      <td>[ Bat,[horseshoe bat],  mink,  white-tailed de...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q3</td>\n",
       "      <td>[cough, fever, amnesia, headache, myalgia, fat...</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q4</td>\n",
       "      <td>[symptomatic treatment, antiviral drug, oxygen...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q5</td>\n",
       "      <td>[tocilizumab, hydroxychloroquine, Acetaminophe...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  QueryID                                             answer  length\n",
       "0      Q1  [Membrane Protein (M Protein), Envelope Protei...       4\n",
       "1      Q2  [ Bat,[horseshoe bat],  mink,  white-tailed de...       5\n",
       "2      Q3  [cough, fever, amnesia, headache, myalgia, fat...      23\n",
       "3      Q4  [symptomatic treatment, antiviral drug, oxygen...       6\n",
       "4      Q5  [tocilizumab, hydroxychloroquine, Acetaminophe...      16"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group the groundtruth by groups\n",
    "gtruth_group = gtruth.groupby(by = ['QueryID']).agg(lambda x: list(x)).reset_index()\n",
    "# data3 = data.fillna('').groupby(by = ['QueryID', 'Main_text']).agg(lambda x: list(x)).reset_index()\n",
    "gtruth_group['length'] = gtruth_group['answer'].apply(lambda x: len(x))\n",
    "gtruth_group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f555df",
   "metadata": {},
   "source": [
    "## Process the answer file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad0fca6b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Evaluation' object has no attribute 'tp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-33877c832964>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mlen_gold_q1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgtruth_group\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgtruth_group\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'QueryID'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'Q1'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'length'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mevaluation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEvaluation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mprecision_q1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprecision\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen_gold_q1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0manws_q1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[0mprecision_q1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-33877c832964>\u001b[0m in \u001b[0;36mprecision\u001b[1;34m(self, len_ground_truth, retrieved_anws)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m#         self.F1()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mprecision\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen_ground_truth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretrieved_anws\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mprecision\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtp\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtp\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mprecision\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrecall\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen_ground_truth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretrieved_anws\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Evaluation' object has no attribute 'tp'"
     ]
    }
   ],
   "source": [
    "class Evaluation:\n",
    "    \n",
    "    def __innit__(self, len_ground_truth, retrieved_anws):\n",
    "        self.len_ground_truth = len_ground_truth\n",
    "        self.retrieved_anws = retrieved_anws        \n",
    "        tp = retrieved_anws['label'].value_counts()[1]\n",
    "        fp = retrieved_anws['label'].value_counts()[0]\n",
    "        fn = len_ground_truth-retrieved_anws['label'].value_counts()[1]\n",
    "#         self.precision ()\n",
    "#         self.recall()\n",
    "#         self.F1()\n",
    "    def precision(self, len_ground_truth, retrieved_anws):\n",
    "        precision = self.tp/(self.tp + self.fp)\n",
    "        return precision\n",
    "    def recall (self, len_ground_truth, retrieved_anws):\n",
    "        recall = self.tp/(self.tp + self.fn)\n",
    "        return recall\n",
    "    def F1 (self, len_ground_truth, retrieved_anws):\n",
    "        f1_score = (2*self.precision*self.recall)/(self.precision+self.recall)\n",
    "        return f1_score\n",
    "        \n",
    "with open(r'C:\\Users\\huyen\\OneDrive\\Documents\\GitHub\\CORD-19-KG\\Results\\KG-output-neo4j-2\\subset_1\\query_1.csv', 'r') as f:\n",
    "    anws_q1 = pd.read_csv(f)\n",
    "# anws['label'].value_counts()[1]\n",
    "\n",
    "len_gold_q1 = gtruth_group[gtruth_group['QueryID']=='Q1']['length'].values[0]\n",
    "evaluation = Evaluation()\n",
    "precision_q1 = evaluation.precision(len_gold_q1,anws_q1)\n",
    "precision_q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ca3098ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder 1 -- Query 1: precision: 0.081, recall: 0.750, F1: 0.081\n",
      "\n",
      "Folder 1 -- Query 2: precision: 0.455, recall: 1.000, F1: 0.455\n",
      "\n",
      "Folder 1 -- Query 3: precision: 1.000, recall: 0.043, F1: 1.000\n",
      "\n",
      "Folder 1 -- Query 4: precision: 0.250, recall: 0.333, F1: 0.250\n",
      "\n",
      "Folder 1 -- Query 5: precision: 0.250, recall: 0.188, F1: 0.250\n",
      "\n",
      "Folder 2 -- Query 1: precision: 0.109, recall: 1.750, F1: 0.109\n",
      "\n",
      "Folder 2 -- Query 2: precision: 0.385, recall: 1.000, F1: 0.385\n",
      "\n",
      "Folder 2 -- Query 3: precision: 0.250, recall: 0.043, F1: 0.250\n",
      "\n",
      "Folder 2 -- Query 4: precision: 0.250, recall: 0.667, F1: 0.250\n",
      "\n",
      "Folder 2 -- Query 5: precision: 0.214, recall: 0.188, F1: 0.214\n",
      "\n",
      "Folder 3 -- Query 1: precision: 0.087, recall: 1.500, F1: 0.087\n",
      "\n",
      "Folder 3 -- Query 2: precision: 0.318, recall: 1.400, F1: 0.318\n",
      "\n",
      "Folder 3 -- Query 3: precision: 0.333, recall: 0.130, F1: 0.333\n",
      "\n",
      "Folder 3 -- Query 4: precision: 0.300, recall: 1.000, F1: 0.300\n",
      "\n",
      "Folder 3 -- Query 5: precision: 0.222, recall: 0.250, F1: 0.222\n",
      "\n",
      "Folder 4 -- Query 1: precision: 0.103, recall: 2.000, F1: 0.103\n",
      "\n",
      "Folder 4 -- Query 2: precision: 0.127, recall: 1.400, F1: 0.127\n",
      "\n",
      "Folder 4 -- Query 3: precision: 0.429, recall: 0.261, F1: 0.429\n",
      "\n",
      "Folder 4 -- Query 4: precision: 0.261, recall: 1.000, F1: 0.261\n",
      "\n",
      "Folder 4 -- Query 5: precision: 0.190, recall: 0.250, F1: 0.190\n",
      "\n",
      "Folder 5 -- Query 1: precision: 0.098, recall: 2.000, F1: 0.098\n",
      "\n",
      "Folder 5 -- Query 2: precision: 0.079, recall: 1.400, F1: 0.079\n",
      "\n",
      "Folder 5 -- Query 3: precision: 0.444, recall: 0.348, F1: 0.444\n",
      "\n",
      "Folder 5 -- Query 4: precision: 0.240, recall: 1.000, F1: 0.240\n",
      "\n",
      "Folder 5 -- Query 5: precision: 0.167, recall: 0.250, F1: 0.167\n",
      "\n",
      "Folder 6 -- Query 1: precision: 0.114, recall: 2.500, F1: 0.114\n",
      "\n",
      "Folder 6 -- Query 2: precision: 0.073, recall: 1.600, F1: 0.073\n",
      "\n",
      "Folder 6 -- Query 3: precision: 0.417, recall: 0.435, F1: 0.417\n",
      "\n",
      "Folder 6 -- Query 4: precision: 0.200, recall: 1.000, F1: 0.200\n",
      "\n",
      "Folder 6 -- Query 5: precision: 0.154, recall: 0.250, F1: 0.154\n",
      "\n",
      "Folder 7 -- Query 1: precision: 0.111, recall: 2.500, F1: 0.111\n",
      "\n",
      "Folder 7 -- Query 2: precision: 0.061, recall: 1.800, F1: 0.061\n",
      "\n",
      "Folder 7 -- Query 3: precision: 0.500, recall: 0.522, F1: 0.500\n",
      "\n",
      "Folder 7 -- Query 4: precision: 0.200, recall: 1.000, F1: 0.200\n",
      "\n",
      "Folder 7 -- Query 5: precision: 0.138, recall: 0.250, F1: 0.138\n",
      "\n",
      "Folder 8 -- Query 1: precision: 0.105, recall: 2.500, F1: 0.105\n",
      "\n",
      "Folder 8 -- Query 2: precision: 0.047, recall: 1.600, F1: 0.047\n",
      "\n",
      "Folder 8 -- Query 3: precision: 0.444, recall: 0.522, F1: 0.444\n",
      "\n",
      "Folder 8 -- Query 4: precision: 0.211, recall: 1.333, F1: 0.211\n",
      "\n",
      "Folder 8 -- Query 5: precision: 0.133, recall: 0.250, F1: 0.133\n",
      "\n",
      "Folder 9 -- Query 1: precision: 0.101, recall: 2.500, F1: 0.101\n",
      "\n",
      "Folder 9 -- Query 2: precision: 0.050, recall: 1.800, F1: 0.050\n",
      "\n",
      "Folder 9 -- Query 3: precision: 0.448, recall: 0.565, F1: 0.448\n",
      "\n",
      "Folder 9 -- Query 4: precision: 0.195, recall: 1.333, F1: 0.195\n",
      "\n",
      "Folder 9 -- Query 5: precision: 0.129, recall: 0.250, F1: 0.129\n",
      "\n",
      "Folder 10 -- Query 1: precision: 0.099, recall: 2.500, F1: 0.099\n",
      "\n",
      "Folder 10 -- Query 2: precision: 0.042, recall: 1.600, F1: 0.042\n",
      "\n",
      "Folder 10 -- Query 3: precision: 0.441, recall: 0.652, F1: 0.441\n",
      "\n",
      "Folder 10 -- Query 4: precision: 0.196, recall: 1.500, F1: 0.196\n",
      "\n",
      "Folder 10 -- Query 5: precision: 0.152, recall: 0.312, F1: 0.152\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# class Evaluation():\n",
    "#     def __innit__(self, len_ground_truth, retrieved_anws):\n",
    "#         self.len_ground_truth = len_ground_truth\n",
    "#         self.retrieved_anws = retrieved_anws\n",
    "        \n",
    "\n",
    "def precision(len_ground_truth, retrieved_anws):\n",
    "    try:\n",
    "        fp = retrieved_anws['label'].value_counts()[0]\n",
    "    except:\n",
    "        fp=0\n",
    "    tp = retrieved_anws['label'].value_counts()[1]\n",
    "    fn = len_ground_truth-retrieved_anws['label'].value_counts()[1]\n",
    "    precision = tp/(tp + fp)\n",
    "    return precision\n",
    "def recall (len_ground_truth, retrieved_anws):\n",
    "    try:\n",
    "        fp = retrieved_anws['label'].value_counts()[0]\n",
    "    except:\n",
    "        fp=0\n",
    "    tp = retrieved_anws['label'].value_counts()[1]\n",
    "    fn = len_ground_truth-retrieved_anws['label'].value_counts()[1]\n",
    "    recall = tp/(tp + fn)\n",
    "    return recall\n",
    "def F1 (len_ground_truth, retrieved_anws):\n",
    "    precision_ = precision(len_ground_truth, retrieved_anws)\n",
    "    recall_ = precision(len_ground_truth, retrieved_anws)\n",
    "    f1_score = (2*precision_*recall_)/(precision_+recall_)\n",
    "    return f1_score\n",
    "\n",
    "\n",
    "############ Calculate the scores and write them to file #######\n",
    "\n",
    "n_folders = 10\n",
    "n_queries = 5\n",
    "all_scores = []\n",
    "\n",
    "with open(r'C:\\Users\\huyen\\OneDrive\\Documents\\GitHub\\CORD-19-KG\\Evaluation\\result\\scores.txt', 'w') as file:\n",
    "            file.write('folder,query,precision,recall,F1\\n')\n",
    "\n",
    "for i in range (1,n_folders+1):\n",
    "    query_scores = {}\n",
    "    for j in range(1,n_queries+1):\n",
    "        query_scores['query_%s'%j]=[]\n",
    "        file_p = r\"C:\\Users\\huyen\\OneDrive\\Documents\\GitHub\\CORD-19-KG\\Results\\KG-output-neo4j-2\\subset_{}\\query_{}.csv\".format(i, j)\n",
    "        with open(file_p, 'r') as f:\n",
    "            anws_q = pd.read_csv(f)\n",
    "        len_gold_q = gtruth_group[gtruth_group['QueryID']=='Q%s'%j]['length'].values[0]\n",
    "        precision_q = precision(len_gold_q,anws_q)\n",
    "        recall_q = recall(len_gold_q,anws_q)\n",
    "        F1_q = F1(len_gold_q,anws_q)\n",
    "        print('Folder {} -- Query {}: precision: {:.3f}, recall: {:.3f}, F1: {:.3f}\\n'.format(i, j, precision_q, recall_q, F1_q))\n",
    "        query_scores['query_%s'%j].append(precision_q)\n",
    "        query_scores['query_%s'%j].append(recall_q)\n",
    "        query_scores['query_%s'%j].append(F1_q)\n",
    "        with open(r'C:\\Users\\huyen\\OneDrive\\Documents\\GitHub\\CORD-19-KG\\Evaluation\\result\\scores.txt', 'a') as file:\n",
    "            file.write('{},{},{:.3f},{:.3f},{:.3f}\\n'.format(i, j, precision_q, recall_q, F1_q))\n",
    "\n",
    "    all_scores.append(folder_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0bc2852f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
