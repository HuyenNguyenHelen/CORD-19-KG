{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d63d9cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hn0139\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dcb54ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'normalized banana covid-19 covid-19 covid-19'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocess the concepts\n",
    "# Defining pre-processing function\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "def preprocessEntity (text):\n",
    "    processed1 = lowercase (text)\n",
    "    processed2 = lemmatization(processed1)\n",
    "    normalized = normalizeCOVID (processed2)\n",
    "    return normalized\n",
    "\n",
    "def lowercase (text):\n",
    "    lowercased = text.lower()\n",
    "    return lowercased\n",
    "\n",
    "def lemmatization(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = ' '.join([lemmatizer.lemmatize(w) for w in text.split()])\n",
    "    return lemmatized\n",
    "\n",
    "def normalizeCOVID (text):\n",
    "    covid_variants1 = ['coronarivus disease', 'sarsr-cov','2019ncov', '2019 ncov','severe acute respiratory syndrome-related coronavirus 2', 'sars-cov2', 'wuhan virus','covid-19 (covid-19)', 'covid19 coronavirus',  'coronarivus', 'sars-cov-2', '2019-ncov', 'covid 19', 'covid19', 'wuhan coronavirus', 'chinese coronavirus', 'covidãƒ¼19', 'novel coronavirus']\n",
    "    covid_variants2 = ['corona','sars-cov']\n",
    "    for variant in covid_variants1:\n",
    "        if variant in text:\n",
    "            text = text.replace(variant, 'covid-19')\n",
    "            \n",
    "    if (('corona' in text) and ('coronavirus' not in text)):\n",
    "            text = text.replace('corona', 'covid-19')\n",
    "            \n",
    "    if (('sars-cov' in text) and ('sars-cov-2' not in text)):\n",
    "            text = text.replace('corona', 'covid-19')\n",
    "    return text\n",
    "        \n",
    "    \n",
    "preprocessEntity ('normalized bananas coronarivus disease coronarivus disease sars-cov-2') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a43150b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all scrapped concepts from Wikidata:  170\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "paths=[r\"C:\\Users\\hn0139\\OneDrive - UNT System\\PROJECTS\\COVID19_paper\\KG paper\\Evaluation\\Evaluation_Students\\comprehensiveness\\all.csv\",\n",
    "r\"C:\\Users\\hn0139\\OneDrive - UNT System\\PROJECTS\\COVID19_paper\\KG paper\\Evaluation\\Evaluation_Students\\comprehensiveness\\COVID-19.csv\",\n",
    "r\"C:\\Users\\hn0139\\OneDrive - UNT System\\PROJECTS\\COVID19_paper\\KG paper\\Evaluation\\Evaluation_Students\\comprehensiveness\\SARS-CoV-2.csv\"]\n",
    "\n",
    "all_entities = []\n",
    "for path in paths: \n",
    "    with open(path, 'r', encoding = 'utf-8' ) as f:\n",
    "        data = pd.read_csv(f)\n",
    "    data = data.fillna('')\n",
    "#     data = data.applymap(preprocessEntity)\n",
    "    data = data.apply(lambda x: x.replace(x,'covid-19 outbreak') if 'outbreak' in x else x)\n",
    "    if len(data.columns) <2:\n",
    "        data ['head'] = data['head'].apply(lambda x: x.replace(x,'covid-19 outbreak') if 'outbreak' in x else x)\n",
    "        data ['head'] = data['head'].apply(lambda x: x.replace(x,'covid-19') if 'wuhan' in x else x)\n",
    "        all_entities.extend( data['head'].values.tolist())\n",
    "        #print('1111-----', len(all_entities))\n",
    "    else:\n",
    "        for col in data.columns:\n",
    "            data [col] = data[col].apply(lambda x: x.replace(x,'covid-19') if 'wuhan' in x else x)\n",
    "            #print('2222---------', len(data[col].values.tolist()))          \n",
    "            all_entities.extend(data[col].values.tolist())\n",
    "        \n",
    "        \n",
    "final = set([item for item in all_entities if type(item) is str ])\n",
    "print('all scrapped concepts from Wikidata: ', len(final))\n",
    "df_final = pd.DataFrame(final, columns = ['concept'])\n",
    "with open(r'C:\\Users\\hn0139\\Documents\\GitHub\\CORD-19-KG\\Evaluation\\groundtruth\\wiki_concepts.csv', 'w', encoding = 'utf-8', newline = '') as file:\n",
    "      df_final.to_csv(file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
